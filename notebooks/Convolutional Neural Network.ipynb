{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "import glob\n",
    "import csv\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "resize_x = 80\n",
    "resize_y = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(\n",
    "        x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(\n",
    "        x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/convolutional/data.csv', 'rb') as mycsvfile:\n",
    "    thedata = csv.reader(mycsvfile, delimiter=':')\n",
    "    thedata.next()\n",
    "    for row in thedata:\n",
    "        image = np.fromstring(row[0].replace(\"[\", \"\").replace(\"]\", \"\"), dtype=int, sep=\" \")\n",
    "        label = row[1]\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        tmp_img = np.fliplr(np.array(image).reshape(80,80,3))\n",
    "        images.append(tmp_img.reshape(-1))\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_labels = pd.get_dummies(labels)\n",
    "labels = np.array(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = np.array(images) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2486, 19200)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images, labels = shuffle(images, labels, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = images[:2000]\n",
    "Y_train = labels.reshape(len(labels), -1)[:2000]\n",
    "\n",
    "X_test = images[2000:]\n",
    "Y_test = labels.reshape(len(labels), -1)[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input and output variables\n",
    "\n",
    "INPUTS = resize_x * resize_y * 3\n",
    "OUTPUTS = 2\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 300\n",
    "LEARNING_RATE = 1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input and output placeholder\n",
    "x = tf.placeholder(tf.float32, [None, INPUTS])\n",
    "y = tf.placeholder(tf.float32, [None, OUTPUTS])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# First Convolutional Layer\n",
    "x_image = tf.reshape(x, [-1, resize_x, resize_y, 3])\n",
    "W_conv_1 = tf.Variable(tf.truncated_normal([2, 2, 3, 8], stddev=0.1))\n",
    "b_conv_1 = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "h_conv_1 = tf.nn.relu(conv2d(x_image, W_conv_1) + b_conv_1)\n",
    "h_pool_1 = max_pool_2x2(h_conv_1)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "W_conv_2 = tf.Variable(tf.truncated_normal([2, 2, 8, 16], stddev=0.1))\n",
    "b_conv_2 = tf.Variable(tf.constant(0.0, shape=[16]))\n",
    "h_conv_2 = tf.nn.relu(conv2d(h_pool_1, W_conv_2) + b_conv_2)\n",
    "h_pool_2 = max_pool_2x2(h_conv_2)\n",
    "\n",
    "# Third Convolutional Layer\n",
    "W_conv_3 = tf.Variable(tf.truncated_normal([2, 2, 16, 32], stddev=0.1))\n",
    "b_conv_3 = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "h_conv_3 = tf.nn.relu(conv2d(h_pool_2, W_conv_3) + b_conv_3)\n",
    "h_pool_3 = max_pool_2x2(h_conv_3)\n",
    "\n",
    "# Fourth Convolutional Layer\n",
    "W_conv_4 = tf.Variable(tf.truncated_normal([2, 2, 32, 64], stddev=0.1))\n",
    "b_conv_4 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "h_conv_4 = tf.nn.relu(conv2d(h_pool_3, W_conv_4) + b_conv_4)\n",
    "h_pool_4 = max_pool_2x2(h_conv_4)\n",
    "\n",
    "# Densely connected layer\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([5 * 5 * 64, 128], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "h_poolfc1_flat = tf.reshape(h_pool_4, [-1, 5 * 5 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_poolfc1_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Densely connected layer\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([128, 256], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.0, shape=[256]))\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "# Dropout\n",
    "h_drop = tf.nn.dropout(h_fc2, pkeep)\n",
    "\n",
    "# Read out Layer\n",
    "W_fc3 = tf.Variable(tf.truncated_normal([256, OUTPUTS], stddev=0.1))\n",
    "b_fc3 = tf.Variable(tf.constant(0.0, shape=[OUTPUTS]))\n",
    "y_logits = tf.matmul(h_drop, W_fc3) + b_fc3\n",
    "y_softmax = tf.nn.softmax(y_logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_logits, y))\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000, loss train 0.67267156, train accuracy 0.59950000, test accuracy 0.63580245\n",
      "Epoch 0001, loss train 0.64331007, train accuracy 0.59950000, test accuracy 0.63580245\n",
      "Epoch 0002, loss train 0.62389052, train accuracy 0.59950000, test accuracy 0.63580245\n",
      "Epoch 0003, loss train 0.62973505, train accuracy 0.59950000, test accuracy 0.63580245\n",
      "Epoch 0004, loss train 0.60471064, train accuracy 0.63450003, test accuracy 0.66255146\n",
      "Epoch 0005, loss train 0.57240498, train accuracy 0.90700001, test accuracy 0.90946501\n",
      "Epoch 0006, loss train 0.47821835, train accuracy 0.88349998, test accuracy 0.89094651\n",
      "Epoch 0007, loss train 0.34041664, train accuracy 0.97649997, test accuracy 0.98353910\n",
      "Epoch 0008, loss train 0.22485557, train accuracy 0.95050001, test accuracy 0.97325104\n",
      "Epoch 0009, loss train 0.25292048, train accuracy 0.98549998, test accuracy 0.98559672\n",
      "Epoch 0010, loss train 0.16187976, train accuracy 0.98850000, test accuracy 0.98559672\n",
      "Epoch 0011, loss train 0.19897446, train accuracy 0.98350000, test accuracy 0.98148149\n",
      "Epoch 0012, loss train 0.10591412, train accuracy 0.98949999, test accuracy 0.98559672\n",
      "Epoch 0013, loss train 0.04484178, train accuracy 0.99100000, test accuracy 0.98559672\n",
      "Epoch 0014, loss train 0.09288695, train accuracy 0.99349999, test accuracy 0.98765433\n",
      "Epoch 0015, loss train 0.11111379, train accuracy 0.99299997, test accuracy 0.98765433\n",
      "Epoch 0016, loss train 0.02750134, train accuracy 0.99400002, test accuracy 0.98765433\n",
      "Epoch 0017, loss train 0.07697919, train accuracy 0.99449998, test accuracy 0.98765433\n",
      "Epoch 0018, loss train 0.16033772, train accuracy 0.99449998, test accuracy 0.98765433\n",
      "Epoch 0019, loss train 0.01920552, train accuracy 0.99500000, test accuracy 0.98765433\n",
      "Epoch 0020, loss train 0.02235659, train accuracy 0.99500000, test accuracy 0.98765433\n",
      "Epoch 0021, loss train 0.05320762, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0022, loss train 0.06068351, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0023, loss train 0.05815216, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0024, loss train 0.02959810, train accuracy 0.99650002, test accuracy 0.99382716\n",
      "Epoch 0025, loss train 0.01634059, train accuracy 0.99500000, test accuracy 0.99382716\n",
      "Epoch 0026, loss train 0.01062789, train accuracy 0.99550003, test accuracy 0.99176955\n",
      "Epoch 0027, loss train 0.09199688, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0028, loss train 0.01280104, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0029, loss train 0.01224974, train accuracy 0.99500000, test accuracy 0.99588478\n",
      "Epoch 0030, loss train 0.01404641, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0031, loss train 0.09277573, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0032, loss train 0.08566725, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0033, loss train 0.02807972, train accuracy 0.99650002, test accuracy 0.99588478\n",
      "Epoch 0034, loss train 0.00528942, train accuracy 0.99550003, test accuracy 0.99382716\n",
      "Epoch 0035, loss train 0.04074359, train accuracy 0.99699998, test accuracy 0.99794239\n",
      "Epoch 0036, loss train 0.01099315, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0037, loss train 0.02808616, train accuracy 0.99699998, test accuracy 0.99794239\n",
      "Epoch 0038, loss train 0.00495231, train accuracy 0.99699998, test accuracy 0.99794239\n",
      "Epoch 0039, loss train 0.00476104, train accuracy 0.99599999, test accuracy 0.99382716\n",
      "Epoch 0040, loss train 0.00885507, train accuracy 0.99699998, test accuracy 0.99794239\n",
      "Epoch 0041, loss train 0.03052462, train accuracy 0.99550003, test accuracy 0.99588478\n",
      "Epoch 0042, loss train 0.00302227, train accuracy 0.99650002, test accuracy 0.99794239\n",
      "Epoch 0043, loss train 0.00460142, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0044, loss train 0.07648946, train accuracy 0.99599999, test accuracy 0.99588478\n",
      "Epoch 0045, loss train 0.00278681, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0046, loss train 0.00306530, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0047, loss train 0.00454699, train accuracy 0.99750000, test accuracy 0.99794239\n",
      "Epoch 0048, loss train 0.00539396, train accuracy 0.99800003, test accuracy 0.99794239\n",
      "Epoch 0049, loss train 0.06790728, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0050, loss train 0.00415079, train accuracy 0.99849999, test accuracy 1.00000000\n",
      "Epoch 0051, loss train 0.00126347, train accuracy 0.99750000, test accuracy 0.99794239\n",
      "Epoch 0052, loss train 0.00183433, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0053, loss train 0.00231000, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0054, loss train 0.00283829, train accuracy 0.99849999, test accuracy 1.00000000\n",
      "Epoch 0055, loss train 0.00757784, train accuracy 0.99750000, test accuracy 0.99794239\n",
      "Epoch 0056, loss train 0.00677186, train accuracy 0.99800003, test accuracy 0.99794239\n",
      "Epoch 0057, loss train 0.08322638, train accuracy 0.99800003, test accuracy 1.00000000\n",
      "Epoch 0058, loss train 0.05527345, train accuracy 0.99599999, test accuracy 0.99794239\n",
      "Epoch 0059, loss train 0.03407898, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0060, loss train 0.03217154, train accuracy 0.99750000, test accuracy 0.99588478\n",
      "Epoch 0061, loss train 0.00152456, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0062, loss train 0.00971838, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0063, loss train 0.00506732, train accuracy 0.99699998, test accuracy 0.99176955\n",
      "Epoch 0064, loss train 0.00146098, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0065, loss train 0.00469815, train accuracy 0.99750000, test accuracy 0.99794239\n",
      "Epoch 0066, loss train 0.00304772, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0067, loss train 0.00161592, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0068, loss train 0.00193673, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0069, loss train 0.00085508, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0070, loss train 0.00199266, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0071, loss train 0.00270458, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0072, loss train 0.00224925, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0073, loss train 0.02708154, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0074, loss train 0.00538885, train accuracy 0.99750000, test accuracy 0.99794239\n",
      "Epoch 0075, loss train 0.00213795, train accuracy 0.99849999, test accuracy 1.00000000\n",
      "Epoch 0076, loss train 0.00078383, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0077, loss train 0.00119887, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0078, loss train 0.00081539, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0079, loss train 0.00166895, train accuracy 0.99750000, test accuracy 0.99382716\n",
      "Epoch 0080, loss train 0.01582130, train accuracy 0.99900001, test accuracy 0.99794239\n",
      "Epoch 0081, loss train 0.00243916, train accuracy 0.99900001, test accuracy 0.99588478\n",
      "Epoch 0082, loss train 0.00211712, train accuracy 0.99849999, test accuracy 1.00000000\n",
      "Epoch 0083, loss train 0.00050831, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0084, loss train 0.00064103, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0085, loss train 0.00077828, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0086, loss train 0.00032492, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0087, loss train 0.00117644, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0088, loss train 0.00363286, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0089, loss train 0.00027399, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0090, loss train 0.00070483, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0091, loss train 0.01703281, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0092, loss train 0.00047919, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0093, loss train 0.00185648, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0094, loss train 0.09068462, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0095, loss train 0.00285986, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0096, loss train 0.03753175, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0097, loss train 0.00051906, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0098, loss train 0.00052420, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0099, loss train 0.00119910, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0100, loss train 0.00031459, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0101, loss train 0.00056533, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0102, loss train 0.01702152, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0103, loss train 0.00057543, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0104, loss train 0.00107672, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0105, loss train 0.00034322, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0106, loss train 0.00163853, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0107, loss train 0.00965508, train accuracy 0.99699998, test accuracy 0.99176955\n",
      "Epoch 0108, loss train 0.00049445, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0109, loss train 0.00098748, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0110, loss train 0.00041103, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0111, loss train 0.00043441, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0112, loss train 0.00020267, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0113, loss train 0.00456296, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0114, loss train 0.00417243, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0115, loss train 0.00023131, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0116, loss train 0.01250922, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0117, loss train 0.00018757, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0118, loss train 0.00422676, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0119, loss train 0.00038450, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0120, loss train 0.00680672, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0121, loss train 0.00018086, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0122, loss train 0.00058686, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0123, loss train 0.00034457, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0124, loss train 0.07636538, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0125, loss train 0.00414633, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0126, loss train 0.00399421, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0127, loss train 0.00020308, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0128, loss train 0.00185338, train accuracy 0.99900001, test accuracy 0.99588478\n",
      "Epoch 0129, loss train 0.00358908, train accuracy 0.99750000, test accuracy 0.99176955\n",
      "Epoch 0130, loss train 0.00304675, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0131, loss train 0.00414555, train accuracy 0.99849999, test accuracy 0.99588478\n",
      "Epoch 0132, loss train 0.01045553, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0133, loss train 0.00014459, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0134, loss train 0.00009101, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0135, loss train 0.00074916, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0136, loss train 0.00028892, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0137, loss train 0.06320041, train accuracy 0.99800003, test accuracy 0.99176955\n",
      "Epoch 0138, loss train 0.00263785, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0139, loss train 0.00037950, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0140, loss train 0.00015372, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0141, loss train 0.00011004, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0142, loss train 0.00021374, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0143, loss train 0.00020959, train accuracy 0.99800003, test accuracy 0.99176955\n",
      "Epoch 0144, loss train 0.00053141, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0145, loss train 0.00007205, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0146, loss train 0.00013133, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0147, loss train 0.00029401, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0148, loss train 0.00012227, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0149, loss train 0.03531459, train accuracy 0.99800003, test accuracy 0.99176955\n",
      "Epoch 0150, loss train 0.00097874, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0151, loss train 0.00015290, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0152, loss train 0.01270807, train accuracy 0.99849999, test accuracy 0.99382716\n",
      "Epoch 0153, loss train 0.00139574, train accuracy 0.99849999, test accuracy 0.99588478\n",
      "Epoch 0154, loss train 0.00038441, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0155, loss train 0.00081885, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0156, loss train 0.00552104, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0157, loss train 0.00044344, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0158, loss train 0.00143810, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0159, loss train 0.00009747, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0160, loss train 0.00017180, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0161, loss train 0.00012628, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0162, loss train 0.00009726, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0163, loss train 0.03577555, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0164, loss train 0.00010787, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0165, loss train 0.00009119, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0166, loss train 0.00027921, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0167, loss train 0.00010872, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0168, loss train 0.00009175, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0169, loss train 0.00004004, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0170, loss train 0.00009431, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0171, loss train 0.00019473, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0172, loss train 0.00005643, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0173, loss train 0.00011503, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0174, loss train 0.00017107, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0175, loss train 0.00016366, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0176, loss train 0.00012264, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0177, loss train 0.00020050, train accuracy 0.99849999, test accuracy 0.99382716\n",
      "Epoch 0178, loss train 0.00042470, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0179, loss train 0.00166760, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0180, loss train 0.00011962, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0181, loss train 0.00103702, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0182, loss train 0.00103449, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0183, loss train 0.00017973, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0184, loss train 0.00031032, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0185, loss train 0.00042677, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0186, loss train 0.00102633, train accuracy 0.99750000, test accuracy 0.98971194\n",
      "Epoch 0187, loss train 0.00984063, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0188, loss train 0.00019445, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0189, loss train 0.00303814, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0190, loss train 0.00008770, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0191, loss train 0.00217928, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0192, loss train 0.00863638, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0193, loss train 0.00020952, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0194, loss train 0.00005071, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0195, loss train 0.00041579, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0196, loss train 0.00010552, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0197, loss train 0.00006099, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0198, loss train 0.00053125, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0199, loss train 0.00004852, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0200, loss train 0.00006557, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0201, loss train 0.00004520, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0202, loss train 0.00254433, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0203, loss train 0.00017148, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0204, loss train 0.00016608, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0205, loss train 0.00002286, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0206, loss train 0.00010246, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0207, loss train 0.00005219, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0208, loss train 0.00051740, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0209, loss train 0.00008385, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0210, loss train 0.00011973, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0211, loss train 0.00021320, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0212, loss train 0.00012987, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0213, loss train 0.00198543, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0214, loss train 0.00005563, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0215, loss train 0.00008180, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0216, loss train 0.00006291, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0217, loss train 0.00004407, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0218, loss train 0.00020644, train accuracy 0.99949998, test accuracy 1.00000000\n",
      "Epoch 0219, loss train 0.05177822, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0220, loss train 0.00009889, train accuracy 1.00000000, test accuracy 0.99794239\n",
      "Epoch 0221, loss train 0.00027215, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0222, loss train 0.01052505, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0223, loss train 0.00002407, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0224, loss train 0.00044612, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0225, loss train 0.00058681, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0226, loss train 0.00006198, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0227, loss train 0.00007810, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0228, loss train 0.00151344, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0229, loss train 0.00015883, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0230, loss train 0.00056445, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0231, loss train 0.00003407, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0232, loss train 0.00016868, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0233, loss train 0.00017682, train accuracy 0.99849999, test accuracy 0.99794239\n",
      "Epoch 0234, loss train 0.00096236, train accuracy 0.99900001, test accuracy 0.99794239\n",
      "Epoch 0235, loss train 0.00014246, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0236, loss train 0.00007643, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0237, loss train 0.00003274, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0238, loss train 0.00002864, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0239, loss train 0.00107538, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0240, loss train 0.00003844, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0241, loss train 0.00232102, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0242, loss train 0.00044962, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0243, loss train 0.00015436, train accuracy 0.99900001, test accuracy 1.00000000\n",
      "Epoch 0244, loss train 0.00007170, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0245, loss train 0.00109017, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0246, loss train 0.00003594, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0247, loss train 0.00002976, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0248, loss train 0.00002065, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0249, loss train 0.00003793, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0250, loss train 0.00004237, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0251, loss train 0.00002991, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0252, loss train 0.00026068, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0253, loss train 0.00059930, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0254, loss train 0.00030288, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0255, loss train 0.00001691, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0256, loss train 0.00022010, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0257, loss train 0.00002933, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0258, loss train 0.00001431, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0259, loss train 0.00001976, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0260, loss train 0.00008838, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0261, loss train 0.00151751, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0262, loss train 0.00003793, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0263, loss train 0.00001877, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0264, loss train 0.00001077, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0265, loss train 0.00000841, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0266, loss train 0.00003324, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0267, loss train 0.00001782, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0268, loss train 0.00003980, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0269, loss train 0.00007826, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0270, loss train 0.00039217, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0271, loss train 0.00002338, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0272, loss train 0.00005548, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0273, loss train 0.00001788, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0274, loss train 0.00003488, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0275, loss train 0.00012689, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0276, loss train 0.00001542, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0277, loss train 0.00001310, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0278, loss train 0.00002628, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0279, loss train 0.00088345, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0280, loss train 0.00007656, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0281, loss train 0.00001441, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0282, loss train 0.00000662, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0283, loss train 0.00138285, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0284, loss train 0.00001753, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0285, loss train 0.00181923, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0286, loss train 0.00000614, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0287, loss train 0.00020709, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0288, loss train 0.00001299, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0289, loss train 0.00003278, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0290, loss train 0.00001669, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0291, loss train 0.00000824, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0292, loss train 0.00001180, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0293, loss train 0.00021592, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0294, loss train 0.00000739, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0295, loss train 0.00002010, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0296, loss train 0.00001914, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0297, loss train 0.00007756, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0298, loss train 0.00018140, train accuracy 1.00000000, test accuracy 1.00000000\n",
      "Epoch 0299, loss train 0.00026029, train accuracy 1.00000000, test accuracy 1.00000000\n"
     ]
    }
   ],
   "source": [
    "loss_train_array = []\n",
    "test_accuracy_array = []\n",
    "train_accuracy_array = []\n",
    "\n",
    "for current_epoch in range(NUM_EPOCHS):\n",
    "    shuffled_index = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(shuffled_index)\n",
    "\n",
    "    train_dataset = X_train[shuffled_index]\n",
    "    train_labels = Y_train[shuffled_index]\n",
    "\n",
    "    for step in xrange(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "        \n",
    "        offset = step * BATCH_SIZE\n",
    "        batch_data = train_dataset[offset:(offset + BATCH_SIZE)]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        \n",
    "        # This dictionary maps the batch data (as a numpy array) to the\n",
    "        # node in the graph is should be fed to.\n",
    "        feed_dict = {x: batch_data, y: batch_labels, pkeep: 0.8}\n",
    "        _, loss_train = sess.run([train_step, loss],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "    # We calculate the accuracies to plot their values later\n",
    "    loss_train_array.append(loss_train)\n",
    "    \n",
    "    train_accuracy = sess.run(\n",
    "        accuracy, feed_dict={x: X_train, y: Y_train, pkeep: 1.0})\n",
    "    \n",
    "    train_accuracy_array.append(train_accuracy)\n",
    "    \n",
    "    test_accuracy = sess.run(\n",
    "        accuracy, feed_dict={x: X_test, y: Y_test, pkeep: 1.0})\n",
    "    \n",
    "    test_accuracy_array.append(test_accuracy)\n",
    "\n",
    "    print (\n",
    "        'Epoch %04d, '\n",
    "        'loss train %.8f, train accuracy %.8f, test accuracy %.8f'\n",
    "        %\n",
    "        (current_epoch,\n",
    "         loss_train,\n",
    "         train_accuracy,\n",
    "         test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /tmp/model_conv_2.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "save_path = saver.save(sess, \"/tmp/model_conv_2.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f771d657b10>]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noBTwIiINSgEvItKgFPAiIg3q/wPBbwzwpjgo8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f771d618550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7715884050>]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76e2113110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracy_array)\n",
    "plt.plot(train_accuracy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-168e3d8d16d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mwin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_overlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m420\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "win = dlib.image_window()\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "#\n",
    "tag = ':)'\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    win.clear_overlay()\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame,'Hola ' + tag + '!!!',(0,130), font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "    \n",
    "    try:\n",
    "        win.set_image(frame)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    win.add_overlay(dlib.rectangle(long(260), long(160), long(420), long(320)))\n",
    "\n",
    "    dets = detector(frame)\n",
    "\n",
    "    for k, d in enumerate(dets):\n",
    "\n",
    "        try:\n",
    "            img = cv2.resize(frame[d.top():d.top() + d.height(), d.left(): d.left() + d.width()], (resize_x, resize_y),\n",
    "                             interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            img = img.reshape(-1).reshape(1, 19200)\n",
    "\n",
    "            img = np.array(img) / 255\n",
    "\n",
    "            feed_dict = {x: img, pkeep: 1.0}\n",
    "\n",
    "            classification = sess.run(y_softmax, feed_dict)\n",
    "\n",
    "            gabriel_level = classification[0][0]\n",
    "            david_level = classification[0][1]\n",
    "\n",
    "            if(gabriel_level > david_level):\n",
    "                tag = 'Gabriel'\n",
    "            else:\n",
    "                tag = 'David'\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
