{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import dlib\n",
    "import glob\n",
    "import csv\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "resize_x = 80\n",
    "resize_y = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(\n",
    "        x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(\n",
    "        x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/convolutional/data.csv', 'rb') as mycsvfile:\n",
    "    thedata = csv.reader(mycsvfile, delimiter=':')\n",
    "    thedata.next()\n",
    "    for row in thedata:\n",
    "        images.append(np.fromstring(row[0].replace(\"[\", \"\").replace(\"]\", \"\"), dtype=int, sep=\" \"))\n",
    "        labels.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_labels = pd.get_dummies(labels)\n",
    "labels = np.array(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = np.array(images) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images, labels = shuffle(images, labels, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = images[:20]\n",
    "Y_train = labels.reshape(len(labels), -1)[:20]\n",
    "\n",
    "X_test = images[20:]\n",
    "Y_test = labels.reshape(len(labels), -1)[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input and output variables\n",
    "\n",
    "INPUTS = resize_x * resize_y * 3\n",
    "OUTPUTS = 2\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 51\n",
    "LEARNING_RATE = 1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interactive session because of notebook environment\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Input and output placeholder\n",
    "x = tf.placeholder(tf.float32, [None, INPUTS])\n",
    "y = tf.placeholder(tf.float32, [None, OUTPUTS])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# First Convolutional Layer\n",
    "x_image = tf.reshape(x, [-1, resize_x, resize_y, 3])\n",
    "W_conv_1 = tf.Variable(tf.truncated_normal([2, 2, 3, 8], stddev=0.1))\n",
    "b_conv_1 = tf.Variable(tf.constant(0.0, shape=[8]))\n",
    "h_conv_1 = tf.nn.relu(conv2d(x_image, W_conv_1) + b_conv_1)\n",
    "h_pool_1 = max_pool_2x2(h_conv_1)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "W_conv_2 = tf.Variable(tf.truncated_normal([2, 2, 8, 32], stddev=0.1))\n",
    "b_conv_2 = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "h_conv_2 = tf.nn.relu(conv2d(h_pool_1, W_conv_2) + b_conv_2)\n",
    "h_pool_2 = max_pool_2x2(h_conv_2)\n",
    "\n",
    "# Third Convolutional Layer\n",
    "W_conv_3 = tf.Variable(tf.truncated_normal([2, 2, 32, 128], stddev=0.1))\n",
    "b_conv_3 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "h_conv_3 = tf.nn.relu(conv2d(h_pool_2, W_conv_3) + b_conv_3)\n",
    "h_pool_3 = max_pool_2x2(h_conv_3)\n",
    "\n",
    "# Fourth Convolutional Layer\n",
    "W_conv_4 = tf.Variable(tf.truncated_normal([2, 2, 128, 256], stddev=0.1))\n",
    "b_conv_4 = tf.Variable(tf.constant(0.0, shape=[256]))\n",
    "h_conv_4 = tf.nn.relu(conv2d(h_pool_3, W_conv_4) + b_conv_4)\n",
    "h_pool_4 = max_pool_2x2(h_conv_4)\n",
    "\n",
    "# Densely connected layer\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([5 * 5 * 256, 1024], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.0, shape=[1024]))\n",
    "h_poolfc1_flat = tf.reshape(h_pool_4, [-1, 5 * 5 * 256])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_poolfc1_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Densely connected layer\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 512], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.0, shape=[512]))\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "# Dropout\n",
    "h_drop = tf.nn.dropout(h_fc2, pkeep)\n",
    "\n",
    "# Read out Layer\n",
    "W_fc3 = tf.Variable(tf.truncated_normal([512, OUTPUTS], stddev=0.1))\n",
    "b_fc3 = tf.Variable(tf.constant(0.0, shape=[OUTPUTS]))\n",
    "y_logits = tf.matmul(h_drop, W_fc3) + b_fc3\n",
    "y_softmax = tf.nn.softmax(y_logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_logits, y))\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_softmax, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000, loss train 0.66675627, train accuracy 0.94999999, test accuracy 0.94789082\n",
      "Epoch 0001, loss train 0.52320278, train accuracy 1.00000000, test accuracy 0.95285362\n",
      "Epoch 0002, loss train 0.32820186, train accuracy 1.00000000, test accuracy 0.95533496\n",
      "Epoch 0003, loss train 0.19390355, train accuracy 1.00000000, test accuracy 0.96526057\n",
      "Epoch 0004, loss train 0.18258622, train accuracy 1.00000000, test accuracy 0.96774191\n",
      "Epoch 0005, loss train 0.11030407, train accuracy 1.00000000, test accuracy 0.96774191\n",
      "Epoch 0006, loss train 0.08197743, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0007, loss train 0.05403546, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0008, loss train 0.04193066, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0009, loss train 0.02526810, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0010, loss train 0.02143857, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0011, loss train 0.01789765, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0012, loss train 0.01366843, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0013, loss train 0.00942010, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0014, loss train 0.00914361, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0015, loss train 0.00690025, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0016, loss train 0.00642154, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0017, loss train 0.00518561, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0018, loss train 0.00407483, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0019, loss train 0.00374038, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0020, loss train 0.00342451, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0021, loss train 0.00347777, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0022, loss train 0.00264436, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0023, loss train 0.00189755, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0024, loss train 0.00231604, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0025, loss train 0.00176270, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0026, loss train 0.00193114, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0027, loss train 0.00126367, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0028, loss train 0.00133175, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0029, loss train 0.00103116, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0030, loss train 0.00106733, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0031, loss train 0.00109990, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0032, loss train 0.00106477, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0033, loss train 0.00095645, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0034, loss train 0.00098850, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0035, loss train 0.00069813, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0036, loss train 0.00080512, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0037, loss train 0.00088231, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0038, loss train 0.00072611, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0039, loss train 0.00081453, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0040, loss train 0.00097500, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0041, loss train 0.00078150, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0042, loss train 0.00074059, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0043, loss train 0.00063412, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0044, loss train 0.00054230, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0045, loss train 0.00052752, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0046, loss train 0.00048724, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0047, loss train 0.00055469, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0048, loss train 0.00053749, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0049, loss train 0.00074753, train accuracy 1.00000000, test accuracy 0.97270471\n",
      "Epoch 0050, loss train 0.00061893, train accuracy 1.00000000, test accuracy 0.97270471\n"
     ]
    }
   ],
   "source": [
    "loss_train_array = []\n",
    "test_accuracy_array = []\n",
    "train_accuracy_array = []\n",
    "\n",
    "for current_epoch in range(NUM_EPOCHS):\n",
    "    shuffled_index = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(shuffled_index)\n",
    "\n",
    "    train_dataset = X_train[shuffled_index]\n",
    "    train_labels = Y_train[shuffled_index]\n",
    "\n",
    "    for step in xrange(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "        \n",
    "        offset = step * BATCH_SIZE\n",
    "        batch_data = train_dataset[offset:(offset + BATCH_SIZE)]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        \n",
    "        # This dictionary maps the batch data (as a numpy array) to the\n",
    "        # node in the graph is should be fed to.\n",
    "        feed_dict = {x: batch_data, y: batch_labels, pkeep: 0.8}\n",
    "        _, loss_train = sess.run([train_step, loss],\n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "    # We calculate the accuracies to plot their values later\n",
    "    loss_train_array.append(loss_train)\n",
    "    \n",
    "    train_accuracy = sess.run(\n",
    "        accuracy, feed_dict={x: X_train, y: Y_train, pkeep: 1.0})\n",
    "    \n",
    "    train_accuracy_array.append(train_accuracy)\n",
    "    \n",
    "    test_accuracy = sess.run(\n",
    "        accuracy, feed_dict={x: X_test, y: Y_test, pkeep: 1.0})\n",
    "    \n",
    "    test_accuracy_array.append(test_accuracy)\n",
    "\n",
    "    print (\n",
    "        'Epoch %04d, '\n",
    "        'loss train %.8f, train accuracy %.8f, test accuracy %.8f'\n",
    "        %\n",
    "        (current_epoch,\n",
    "         loss_train,\n",
    "         train_accuracy,\n",
    "         test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /tmp/model_conv.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Save model\n",
    "save_path = saver.save(sess, \"/tmp/model_conv.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n",
      "[[  2.25983822e-04   9.99774039e-01]\n",
      " [  9.99717772e-01   2.82169058e-04]\n",
      " [  9.99723256e-01   2.76717154e-04]\n",
      " [  1.16525043e-05   9.99988317e-01]\n",
      " [  9.99143839e-01   8.56134749e-04]\n",
      " [  9.99752343e-01   2.47671473e-04]\n",
      " [  3.60743230e-04   9.99639273e-01]\n",
      " [  9.99675155e-01   3.24914115e-04]\n",
      " [  5.04815194e-04   9.99495149e-01]\n",
      " [  1.95411616e-04   9.99804556e-01]\n",
      " [  5.17757842e-04   9.99482214e-01]\n",
      " [  4.36798378e-04   9.99563158e-01]\n",
      " [  5.20068686e-04   9.99479949e-01]\n",
      " [  1.93891945e-04   9.99806106e-01]\n",
      " [  2.50383862e-04   9.99749601e-01]\n",
      " [  9.99612272e-01   3.87803593e-04]\n",
      " [  9.99679089e-01   3.20932828e-04]\n",
      " [  5.78788109e-04   9.99421239e-01]\n",
      " [  3.01847846e-04   9.99698162e-01]\n",
      " [  2.87870760e-04   9.99712050e-01]\n",
      " [  9.99650836e-01   3.49132053e-04]\n",
      " [  2.74225808e-04   9.99725759e-01]\n",
      " [  2.12270359e-04   9.99787748e-01]\n",
      " [  9.99789059e-01   2.10991668e-04]\n",
      " [  9.99754012e-01   2.45961623e-04]\n",
      " [  9.98985708e-01   1.01425685e-03]\n",
      " [  2.14659865e-03   9.97853339e-01]\n",
      " [  9.99653459e-01   3.46494635e-04]\n",
      " [  9.99713123e-01   2.86808383e-04]\n",
      " [  4.57638875e-04   9.99542356e-01]\n",
      " [  6.11550990e-04   9.99388456e-01]\n",
      " [  9.99713838e-01   2.86129158e-04]\n",
      " [  3.27123445e-03   9.96728778e-01]\n",
      " [  3.88207845e-04   9.99611795e-01]\n",
      " [  9.99665737e-01   3.34323529e-04]\n",
      " [  2.93885387e-04   9.99706089e-01]\n",
      " [  9.99481499e-01   5.18561457e-04]\n",
      " [  9.99052465e-01   9.47576191e-04]\n",
      " [  9.99686837e-01   3.13205906e-04]\n",
      " [  9.99651909e-01   3.48058646e-04]\n",
      " [  9.99652147e-01   3.47848487e-04]\n",
      " [  1.54625304e-04   9.99845386e-01]\n",
      " [  4.06164589e-04   9.99593794e-01]\n",
      " [  1.53827772e-01   8.46172214e-01]\n",
      " [  9.99221087e-01   7.78933580e-04]\n",
      " [  4.76795435e-02   9.52320457e-01]\n",
      " [  2.83335015e-04   9.99716699e-01]\n",
      " [  9.99735892e-01   2.64072616e-04]\n",
      " [  9.99212503e-01   7.87522236e-04]\n",
      " [  1.69220395e-04   9.99830723e-01]\n",
      " [  7.57897564e-04   9.99242067e-01]\n",
      " [  3.86959378e-04   9.99613106e-01]\n",
      " [  2.22756353e-04   9.99777257e-01]\n",
      " [  9.99641776e-01   3.58207413e-04]\n",
      " [  2.54419429e-04   9.99745548e-01]\n",
      " [  9.99643683e-01   3.56304226e-04]\n",
      " [  9.99587357e-01   4.12692083e-04]\n",
      " [  2.97497201e-04   9.99702513e-01]\n",
      " [  2.57287640e-04   9.99742687e-01]\n",
      " [  9.99622107e-01   3.77875229e-04]\n",
      " [  3.44719389e-04   9.99655247e-01]\n",
      " [  9.99680400e-01   3.19588784e-04]\n",
      " [  9.52593386e-01   4.74065393e-02]\n",
      " [  9.99702275e-01   2.97732971e-04]\n",
      " [  3.36860918e-04   9.99663115e-01]\n",
      " [  9.99217749e-01   7.82250776e-04]\n",
      " [  9.99662161e-01   3.37888720e-04]\n",
      " [  8.79729458e-04   9.99120295e-01]\n",
      " [  2.11344624e-04   9.99788702e-01]\n",
      " [  8.40127352e-04   9.99159932e-01]\n",
      " [  2.56330299e-04   9.99743640e-01]\n",
      " [  9.99714315e-01   2.85620859e-04]\n",
      " [  9.99697924e-01   3.02057393e-04]\n",
      " [  7.16769078e-04   9.99283254e-01]\n",
      " [  9.99532938e-01   4.67129867e-04]\n",
      " [  7.81808863e-04   9.99218225e-01]\n",
      " [  2.15380359e-03   9.97846246e-01]\n",
      " [  9.99656677e-01   3.43270018e-04]\n",
      " [  3.35620745e-04   9.99664426e-01]\n",
      " [  3.78392200e-04   9.99621630e-01]\n",
      " [  2.99253501e-04   9.99700785e-01]\n",
      " [  3.10720701e-04   9.99689341e-01]\n",
      " [  9.99184191e-01   8.15792300e-04]\n",
      " [  4.25777631e-04   9.99574244e-01]\n",
      " [  2.78658321e-04   9.99721348e-01]\n",
      " [  2.09381133e-01   7.90618837e-01]\n",
      " [  1.71638138e-04   9.99828339e-01]\n",
      " [  9.99641776e-01   3.58243968e-04]\n",
      " [  2.42766473e-04   9.99757230e-01]\n",
      " [  7.37216673e-04   9.99262750e-01]\n",
      " [  3.86684987e-04   9.99613345e-01]\n",
      " [  1.78793052e-04   9.99821246e-01]\n",
      " [  9.99639034e-01   3.60900536e-04]\n",
      " [  2.78038380e-04   9.99721944e-01]\n",
      " [  2.52185226e-03   9.97478187e-01]\n",
      " [  9.99685645e-01   3.14390607e-04]\n",
      " [  9.99723017e-01   2.76986131e-04]\n",
      " [  2.33401646e-04   9.99766648e-01]\n",
      " [  2.48217286e-04   9.99751747e-01]\n",
      " [  2.08543956e-01   7.91456044e-01]\n",
      " [  9.99725401e-01   2.74549413e-04]\n",
      " [  9.99662280e-01   3.37718870e-04]\n",
      " [  1.03780894e-05   9.99989629e-01]\n",
      " [  3.83665931e-04   9.99616265e-01]\n",
      " [  9.99081969e-01   9.18009842e-04]\n",
      " [  9.99684930e-01   3.15035664e-04]\n",
      " [  9.99675274e-01   3.24783701e-04]\n",
      " [  9.99645829e-01   3.54176416e-04]\n",
      " [  1.52904890e-04   9.99847054e-01]\n",
      " [  3.46586196e-04   9.99653459e-01]\n",
      " [  9.99329925e-01   6.70067209e-04]\n",
      " [  9.99694705e-01   3.05336434e-04]\n",
      " [  9.99670386e-01   3.29607079e-04]\n",
      " [  3.33767792e-04   9.99666214e-01]\n",
      " [  3.13070981e-04   9.99686956e-01]\n",
      " [  3.60861607e-03   9.96391356e-01]\n",
      " [  9.99734104e-01   2.65850569e-04]\n",
      " [  9.99790490e-01   2.09515594e-04]\n",
      " [  9.99316931e-01   6.83052058e-04]\n",
      " [  9.99735534e-01   2.64484610e-04]\n",
      " [  9.99672055e-01   3.28007474e-04]\n",
      " [  4.93777217e-04   9.99506235e-01]\n",
      " [  9.99726713e-01   2.73222104e-04]\n",
      " [  9.99658942e-01   3.41001345e-04]\n",
      " [  1.98217283e-04   9.99801815e-01]\n",
      " [  3.29632545e-04   9.99670386e-01]\n",
      " [  1.90620965e-04   9.99809444e-01]\n",
      " [  3.58081772e-04   9.99641895e-01]\n",
      " [  9.99612510e-01   3.87536711e-04]\n",
      " [  4.23875870e-04   9.99576151e-01]\n",
      " [  9.99728858e-01   2.71175057e-04]\n",
      " [  9.99453843e-01   5.46117662e-04]\n",
      " [  8.24603077e-04   9.99175370e-01]\n",
      " [  4.76384885e-04   9.99523640e-01]\n",
      " [  9.99626040e-01   3.73905175e-04]\n",
      " [  1.39343669e-03   9.98606622e-01]\n",
      " [  2.22507340e-04   9.99777496e-01]\n",
      " [  9.98981535e-01   1.01841893e-03]\n",
      " [  9.99000490e-01   9.99481184e-04]\n",
      " [  9.99710262e-01   2.89679039e-04]\n",
      " [  9.98955488e-01   1.04443962e-03]\n",
      " [  2.33358907e-04   9.99766648e-01]\n",
      " [  9.99676824e-01   3.23193904e-04]\n",
      " [  4.06096253e-04   9.99593914e-01]\n",
      " [  5.45232208e-04   9.99454796e-01]\n",
      " [  9.99659061e-01   3.40965576e-04]\n",
      " [  9.99710143e-01   2.89820222e-04]\n",
      " [  5.73777943e-04   9.99426246e-01]\n",
      " [  2.98894796e-04   9.99701083e-01]\n",
      " [  2.35947027e-05   9.99976397e-01]\n",
      " [  9.99297023e-01   7.02993304e-04]\n",
      " [  9.99780715e-01   2.19278911e-04]\n",
      " [  9.99714792e-01   2.85220012e-04]\n",
      " [  9.99740183e-01   2.59806926e-04]\n",
      " [  9.99078274e-01   9.21746716e-04]\n",
      " [  9.99689221e-01   3.10764241e-04]\n",
      " [  3.71000060e-04   9.99629021e-01]\n",
      " [  4.18270647e-04   9.99581754e-01]\n",
      " [  9.99669075e-01   3.30908340e-04]\n",
      " [  9.99118388e-01   8.81573767e-04]\n",
      " [  9.99749720e-01   2.50325887e-04]\n",
      " [  9.99567211e-01   4.32833622e-04]\n",
      " [  9.98957515e-01   1.04252063e-03]\n",
      " [  9.99602973e-01   3.97017982e-04]\n",
      " [  2.92902459e-05   9.99970675e-01]\n",
      " [  2.19603840e-04   9.99780357e-01]\n",
      " [  2.02838186e-04   9.99797165e-01]\n",
      " [  9.99647021e-01   3.52970033e-04]\n",
      " [  2.19429712e-04   9.99780595e-01]\n",
      " [  9.99696732e-01   3.03302309e-04]\n",
      " [  9.99481976e-01   5.18059009e-04]\n",
      " [  3.15153447e-04   9.99684811e-01]\n",
      " [  9.98949826e-01   1.05016073e-03]\n",
      " [  1.55110567e-04   9.99844909e-01]\n",
      " [  3.22236470e-03   9.96777594e-01]\n",
      " [  9.99618769e-01   3.81185411e-04]\n",
      " [  5.07454643e-05   9.99949217e-01]\n",
      " [  2.28344201e-04   9.99771655e-01]\n",
      " [  9.99623895e-01   3.76089825e-04]\n",
      " [  9.99002755e-01   9.97291994e-04]\n",
      " [  9.99632955e-01   3.67003202e-04]\n",
      " [  2.75128754e-04   9.99724805e-01]\n",
      " [  9.99273479e-01   7.26486789e-04]\n",
      " [  9.99603093e-01   3.96965363e-04]\n",
      " [  2.21789523e-04   9.99778211e-01]\n",
      " [  5.32727689e-04   9.99467313e-01]\n",
      " [  9.99687433e-01   3.12592892e-04]\n",
      " [  4.87987330e-04   9.99511957e-01]\n",
      " [  9.99773085e-01   2.26866134e-04]\n",
      " [  9.99267638e-01   7.32384331e-04]\n",
      " [  9.99644160e-01   3.55879922e-04]\n",
      " [  9.99697924e-01   3.02108092e-04]\n",
      " [  9.99663115e-01   3.36833298e-04]\n",
      " [  9.99571860e-01   4.28149680e-04]\n",
      " [  9.99670744e-01   3.29301780e-04]\n",
      " [  9.99204218e-01   7.95862346e-04]\n",
      " [  2.65974225e-03   9.97340262e-01]\n",
      " [  5.18539222e-04   9.99481499e-01]\n",
      " [  1.71209249e-04   9.99828815e-01]\n",
      " [  9.99680400e-01   3.19655257e-04]\n",
      " [  7.35079870e-04   9.99264896e-01]\n",
      " [  9.99842882e-01   1.57094211e-04]\n",
      " [  2.50552956e-04   9.99749482e-01]\n",
      " [  1.87797309e-03   9.98122036e-01]\n",
      " [  2.44509603e-04   9.99755442e-01]\n",
      " [  1.34204421e-03   9.98657942e-01]\n",
      " [  9.99621987e-01   3.77957011e-04]\n",
      " [  9.99486804e-01   5.13145817e-04]\n",
      " [  9.99248326e-01   7.51624932e-04]\n",
      " [  2.64927326e-03   9.97350693e-01]\n",
      " [  1.09560497e-04   9.99890447e-01]\n",
      " [  1.61021564e-03   9.98389840e-01]\n",
      " [  9.20158200e-05   9.99907970e-01]\n",
      " [  9.99676704e-01   3.23267857e-04]\n",
      " [  3.87509383e-04   9.99612510e-01]\n",
      " [  2.16298897e-04   9.99783695e-01]\n",
      " [  7.18666066e-04   9.99281347e-01]\n",
      " [  9.99095678e-01   9.04370099e-04]\n",
      " [  4.32505680e-04   9.99567449e-01]\n",
      " [  9.98916984e-01   1.08306040e-03]\n",
      " [  9.99855399e-01   1.44528065e-04]\n",
      " [  9.99688148e-01   3.11836542e-04]\n",
      " [  9.99210119e-01   7.89903279e-04]\n",
      " [  3.90786969e-04   9.99609292e-01]\n",
      " [  9.99664903e-01   3.35062185e-04]\n",
      " [  4.76570392e-04   9.99523401e-01]\n",
      " [  5.98837596e-06   9.99994040e-01]\n",
      " [  9.99651670e-01   3.48355243e-04]\n",
      " [  2.08615791e-04   9.99791443e-01]\n",
      " [  4.11839254e-04   9.99588192e-01]\n",
      " [  9.99254286e-01   7.45693862e-04]\n",
      " [  1.54793415e-05   9.99984503e-01]\n",
      " [  9.99745309e-01   2.54640268e-04]\n",
      " [  3.27046320e-04   9.99673009e-01]\n",
      " [  3.09178198e-04   9.99690890e-01]\n",
      " [  9.99239445e-01   7.60601426e-04]\n",
      " [  3.73952236e-04   9.99626040e-01]\n",
      " [  9.99645591e-01   3.54436517e-04]\n",
      " [  9.99053776e-01   9.46272979e-04]\n",
      " [  9.99397755e-01   6.02237007e-04]\n",
      " [  1.04408782e-05   9.99989510e-01]\n",
      " [  3.51871538e-04   9.99648094e-01]\n",
      " [  9.99741495e-01   2.58543238e-04]\n",
      " [  9.99254405e-01   7.45603582e-04]\n",
      " [  2.42797032e-04   9.99757230e-01]\n",
      " [  9.99714077e-01   2.85957911e-04]\n",
      " [  9.99708712e-01   2.91269796e-04]\n",
      " [  2.46853189e-04   9.99753177e-01]\n",
      " [  9.99165058e-01   8.34948616e-04]\n",
      " [  2.71774316e-03   9.97282267e-01]\n",
      " [  2.98420433e-04   9.99701560e-01]\n",
      " [  9.99729931e-01   2.70049874e-04]\n",
      " [  5.70205098e-04   9.99429762e-01]\n",
      " [  9.99703586e-01   2.96384620e-04]\n",
      " [  2.50918732e-04   9.99749124e-01]\n",
      " [  2.39254892e-04   9.99760807e-01]\n",
      " [  9.99697566e-01   3.02508764e-04]\n",
      " [  9.99026895e-01   9.73168237e-04]\n",
      " [  2.11527251e-04   9.99788463e-01]\n",
      " [  4.25488892e-04   9.99574482e-01]\n",
      " [  2.15473145e-01   7.84526825e-01]\n",
      " [  9.99233961e-01   7.66023004e-04]\n",
      " [  7.16564886e-04   9.99283493e-01]\n",
      " [  9.99715745e-01   2.84292648e-04]\n",
      " [  9.99739468e-01   2.60484783e-04]\n",
      " [  3.90422792e-04   9.99609649e-01]\n",
      " [  9.99646425e-01   3.53572890e-04]\n",
      " [  9.98983920e-01   1.01606885e-03]\n",
      " [  9.99737084e-01   2.62886198e-04]\n",
      " [  3.43228981e-04   9.99656796e-01]\n",
      " [  7.91925413e-04   9.99208152e-01]\n",
      " [  2.21372713e-04   9.99778688e-01]\n",
      " [  2.02415089e-04   9.99797642e-01]\n",
      " [  2.48874916e-04   9.99751151e-01]\n",
      " [  9.99338210e-01   6.61773433e-04]\n",
      " [  9.99602139e-01   3.97859840e-04]\n",
      " [  2.01168674e-04   9.99798834e-01]\n",
      " [  9.98767376e-01   1.23264384e-03]\n",
      " [  9.99705017e-01   2.94977654e-04]\n",
      " [  4.33665205e-04   9.99566376e-01]\n",
      " [  2.91019707e-04   9.99708951e-01]\n",
      " [  9.99753058e-01   2.46970652e-04]\n",
      " [  9.99583185e-01   4.16874042e-04]\n",
      " [  3.49076174e-04   9.99650955e-01]\n",
      " [  9.99748290e-01   2.51685909e-04]\n",
      " [  3.13788129e-04   9.99686241e-01]\n",
      " [  9.99167919e-01   8.32146092e-04]\n",
      " [  2.63140450e-04   9.99736845e-01]\n",
      " [  9.99611557e-01   3.88420682e-04]\n",
      " [  4.36770119e-04   9.99563277e-01]\n",
      " [  9.99733865e-01   2.66151124e-04]\n",
      " [  4.54471854e-04   9.99545515e-01]\n",
      " [  4.02022211e-04   9.99597967e-01]\n",
      " [  3.70531721e-04   9.99629498e-01]\n",
      " [  9.99342620e-01   6.57393946e-04]\n",
      " [  9.99670982e-01   3.29061731e-04]\n",
      " [  9.99640703e-01   3.59321712e-04]\n",
      " [  9.99648333e-01   3.51699709e-04]\n",
      " [  2.40501110e-03   9.97595012e-01]\n",
      " [  2.85372022e-04   9.99714553e-01]\n",
      " [  2.57023814e-04   9.99742925e-01]\n",
      " [  9.99441564e-01   5.58437081e-04]\n",
      " [  9.99665380e-01   3.34658689e-04]\n",
      " [  3.42367188e-04   9.99657631e-01]\n",
      " [  9.99746501e-01   2.53460923e-04]\n",
      " [  9.99717295e-01   2.82696856e-04]\n",
      " [  4.31034685e-04   9.99568999e-01]\n",
      " [  4.37458453e-04   9.99562562e-01]\n",
      " [  9.99192297e-01   8.07675940e-04]\n",
      " [  5.52284648e-04   9.99447763e-01]\n",
      " [  1.97398913e-04   9.99802649e-01]\n",
      " [  9.99708354e-01   2.91597069e-04]\n",
      " [  2.69797922e-04   9.99730170e-01]\n",
      " [  2.68679752e-04   9.99731362e-01]\n",
      " [  9.99412298e-01   5.87698712e-04]\n",
      " [  2.44179246e-04   9.99755800e-01]\n",
      " [  1.49218453e-04   9.99850750e-01]\n",
      " [  2.36118925e-04   9.99763906e-01]\n",
      " [  9.99271810e-01   7.28213461e-04]\n",
      " [  9.99727547e-01   2.72397825e-04]\n",
      " [  9.99673367e-01   3.26606969e-04]\n",
      " [  2.78841937e-04   9.99721110e-01]\n",
      " [  1.18634384e-03   9.98813629e-01]\n",
      " [  2.24288669e-04   9.99775708e-01]\n",
      " [  9.99098659e-01   9.01390100e-04]\n",
      " [  9.99310017e-01   6.90008630e-04]\n",
      " [  9.99636889e-01   3.63103056e-04]\n",
      " [  9.99612272e-01   3.87775479e-04]\n",
      " [  1.94085063e-04   9.99805987e-01]\n",
      " [  7.38614646e-04   9.99261320e-01]\n",
      " [  1.78456859e-04   9.99821603e-01]\n",
      " [  1.11523907e-04   9.99888420e-01]\n",
      " [  1.99111248e-03   9.98008907e-01]\n",
      " [  2.24289513e-04   9.99775708e-01]\n",
      " [  2.85971531e-04   9.99713957e-01]\n",
      " [  9.99578893e-01   4.21114179e-04]\n",
      " [  2.53080670e-03   9.97469187e-01]\n",
      " [  9.99501228e-01   4.98762121e-04]\n",
      " [  9.99677062e-01   3.22929642e-04]\n",
      " [  9.99258220e-01   7.41753553e-04]\n",
      " [  4.48732259e-04   9.99551237e-01]\n",
      " [  5.22520801e-04   9.99477446e-01]\n",
      " [  9.99691486e-01   3.08549497e-04]\n",
      " [  9.99682903e-01   3.17145750e-04]\n",
      " [  9.99290824e-01   7.09140324e-04]\n",
      " [  9.99012947e-01   9.87032778e-04]\n",
      " [  3.81617050e-04   9.99618411e-01]\n",
      " [  2.23168143e-04   9.99776900e-01]\n",
      " [  2.07506686e-01   7.92493284e-01]\n",
      " [  9.99745548e-01   2.54419429e-04]\n",
      " [  2.85373942e-04   9.99714553e-01]\n",
      " [  2.45948002e-04   9.99754012e-01]\n",
      " [  2.19934504e-04   9.99780118e-01]\n",
      " [  9.99081612e-01   9.18388308e-04]\n",
      " [  9.99309063e-01   6.90941059e-04]\n",
      " [  1.81887731e-01   8.18112254e-01]\n",
      " [  8.25396564e-05   9.99917507e-01]\n",
      " [  9.99577701e-01   4.22350859e-04]\n",
      " [  2.85698217e-04   9.99714315e-01]\n",
      " [  9.99736011e-01   2.64003174e-04]\n",
      " [  2.64129776e-04   9.99735892e-01]\n",
      " [  9.99727309e-01   2.72709149e-04]\n",
      " [  7.04591163e-04   9.99295354e-01]\n",
      " [  9.99682784e-01   3.17202910e-04]\n",
      " [  5.46456606e-04   9.99453485e-01]\n",
      " [  3.10553645e-04   9.99689460e-01]\n",
      " [  9.58637774e-01   4.13622409e-02]\n",
      " [  2.28232559e-04   9.99771774e-01]\n",
      " [  3.28373804e-04   9.99671578e-01]\n",
      " [  9.99105632e-01   8.94318568e-04]\n",
      " [  9.99196112e-01   8.03871430e-04]\n",
      " [  9.99656558e-01   3.43425694e-04]\n",
      " [  2.85019923e-04   9.99714911e-01]\n",
      " [  2.30689198e-04   9.99769270e-01]\n",
      " [  9.99701440e-01   2.98493833e-04]\n",
      " [  6.36866724e-04   9.99363124e-01]\n",
      " [  3.83872684e-04   9.99616146e-01]\n",
      " [  2.58061511e-04   9.99741971e-01]\n",
      " [  9.99706089e-01   2.93882884e-04]\n",
      " [  2.54610740e-03   9.97453868e-01]\n",
      " [  2.88089621e-04   9.99711931e-01]\n",
      " [  9.99541163e-01   4.58822586e-04]\n",
      " [  2.13163148e-04   9.99786794e-01]\n",
      " [  9.99605000e-01   3.95062321e-04]\n",
      " [  3.89141554e-04   9.99610841e-01]\n",
      " [  9.99204099e-01   7.95963977e-04]\n",
      " [  3.41521401e-04   9.99658465e-01]\n",
      " [  1.79530343e-05   9.99981999e-01]\n",
      " [  2.54660670e-04   9.99745309e-01]\n",
      " [  3.00072570e-04   9.99699950e-01]\n",
      " [  2.76240055e-04   9.99723732e-01]\n",
      " [  2.24934367e-04   9.99775112e-01]\n",
      " [  3.41901090e-04   9.99658108e-01]\n",
      " [  9.99363124e-01   6.36897981e-04]\n",
      " [  9.99615312e-01   3.84668354e-04]\n",
      " [  1.67738297e-03   9.98322546e-01]\n",
      " [  9.99728858e-01   2.71076540e-04]\n",
      " [  9.98747230e-01   1.25271734e-03]\n",
      " [  7.22385361e-04   9.99277651e-01]\n",
      " [  3.37728852e-04   9.99662280e-01]\n",
      " [  9.61320067e-04   9.99038696e-01]\n",
      " [  2.05337506e-04   9.99794662e-01]\n",
      " [  2.04423512e-03   9.97955799e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Comparing values\n",
    "print Y_test\n",
    "\n",
    "classification = sess.run(y_softmax, feed_dict={x: X_test, pkeep: 1.0})\n",
    "print classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
